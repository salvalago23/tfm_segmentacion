{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e180692f",
   "metadata": {},
   "source": [
    "# Notebook 03: An√°lisis Comparativo de Modelos de Segmentaci√≥n\n",
    "\n",
    "**Objetivos:**\n",
    "1. Cargar y comparar modelos entrenados desde diferentes experimentos\n",
    "2. An√°lisis estad√≠stico detallado de rendimiento\n",
    "3. Visualizaciones cualitativas y cuantitativas para la memoria del TFM\n",
    "4. An√°lisis de errores y casos l√≠mite\n",
    "5. Conclusiones y recomendaciones finales\n",
    "\n",
    "**Metodolog√≠a:**\n",
    "- Carga de modelos desde experimentos anteriores\n",
    "- Evaluaci√≥n en conjunto de validaci√≥n y test (si disponible)\n",
    "- An√°lisis estad√≠stico (distribuciones, correlaciones, significancia)\n",
    "- Visualizaci√≥n de segmentaciones representativas\n",
    "- Comparaci√≥n con referencias del estado del arte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116ec0e",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n inicial y carga de librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ec0650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n inicial\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import json\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import itertools\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de estilo para gr√°ficos acad√©micos\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"colorblind\")\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (14, 8),\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 16,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 12,\n",
    "    'figure.titlesize': 18,\n",
    "    'figure.titleweight': 'bold',\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'savefig.transparent': False\n",
    "})\n",
    "\n",
    "# A√±adir src al path\n",
    "sys.path.append(str(Path.cwd()))\n",
    "\n",
    "# Importaciones personalizadas\n",
    "from src.data_preparation.data_loader import MedicalDataLoader\n",
    "from src.models import UNet, AttentionUNet, ResidualUNet\n",
    "from src.models.metrics import SegmentationMetrics\n",
    "\n",
    "# Configurar dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üì± Dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc813c",
   "metadata": {},
   "source": [
    "## 2. Carga de modelos entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_models(model_paths):\n",
    "    \"\"\"\n",
    "    Carga m√∫ltiples modelos entrenados desde sus checkpoints\n",
    "    \n",
    "    Args:\n",
    "        model_paths: Diccionario con {nombre_modelo: ruta_checkpoint}\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con modelos cargados y su metadata\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    for name, path in model_paths.items():\n",
    "        path = Path(path)\n",
    "        if not path.exists():\n",
    "            print(f\"‚ö†Ô∏è  No encontrado: {path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"üìÇ Cargando {name} desde {path}\")\n",
    "        \n",
    "        try:\n",
    "            # Cargar checkpoint\n",
    "            checkpoint = torch.load(path, map_location=device, weights_only=False)\n",
    "        except:\n",
    "            try:\n",
    "                checkpoint = torch.load(path, map_location=device)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error cargando {name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Determinar tipo de modelo\n",
    "        if 'attention' in name.lower() or 'Attention' in str(path):\n",
    "            model_class = AttentionUNet\n",
    "        elif 'residual' in name.lower() or 'Residual' in str(path):\n",
    "            model_class = ResidualUNet\n",
    "        else:\n",
    "            model_class = UNet\n",
    "        \n",
    "        # Crear e inicializar modelo\n",
    "        model = model_class(in_channels=3, out_channels=1)\n",
    "        \n",
    "        # Cargar pesos\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        elif 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Extraer metadata\n",
    "        metadata = {\n",
    "            'checkpoint': checkpoint,\n",
    "            'model_class': model.__class__.__name__,\n",
    "            'params': sum(p.numel() for p in model.parameters()),\n",
    "            'path': path\n",
    "        }\n",
    "        \n",
    "        # A√±adir informaci√≥n de entrenamiento si est√° disponible\n",
    "        if 'history' in checkpoint:\n",
    "            metadata['history'] = checkpoint['history']\n",
    "        if 'best_val_dice' in checkpoint:\n",
    "            metadata['best_val_dice'] = checkpoint['best_val_dice']\n",
    "        \n",
    "        models[name] = {\n",
    "            'model': model,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ {name} cargado: {metadata['params']:,} par√°metros\")\n",
    "    \n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir rutas de modelos entrenados (ajustar seg√∫n tus experimentos)\n",
    "MODEL_PATHS = {\n",
    "    'U-Net B√°sica': 'experiments/test_unet_5epochs/final_model.pth',\n",
    "    # 'Attention U-Net': 'experiments/attention_unet_5epochs/final_model.pth',\n",
    "    # 'Residual U-Net': 'experiments/residual_unet_5epochs/final_model.pth'\n",
    "}\n",
    "\n",
    "print(\"üß† CARGANDO MODELOS ENTRENADOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Cargar modelos\n",
    "trained_models = load_trained_models(MODEL_PATHS)\n",
    "\n",
    "if not trained_models:\n",
    "    print(\"‚ùå No se pudieron cargar modelos. Verifica las rutas.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Modelos cargados exitosamente: {len(trained_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac336a70",
   "metadata": {},
   "source": [
    "## 3. Preparaci√≥n de datos para evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c04e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä PREPARANDO DATOS PARA EVALUACI√ìN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Configuraci√≥n de evaluaci√≥n\n",
    "EVAL_CONFIG = {\n",
    "    'batch_size': 8,\n",
    "    'target_size': (256, 256),\n",
    "    'threshold': 0.5  # Umbral para binarizaci√≥n\n",
    "}\n",
    "\n",
    "# Crear DataLoader para evaluaci√≥n\n",
    "print(\"Creando DataLoader...\")\n",
    "eval_loader = MedicalDataLoader(\n",
    "    base_path=\"data\",\n",
    "    batch_size=EVAL_CONFIG['batch_size'],\n",
    "    target_size=EVAL_CONFIG['target_size'],\n",
    "    num_workers=2,\n",
    "    use_class_balancing=False  # No balancear en evaluaci√≥n\n",
    ")\n",
    "\n",
    "eval_loader.create_datasets(use_processed=True, augment_train=False)\n",
    "eval_loader.create_dataloaders(shuffle_train=False)\n",
    "\n",
    "print(f\"‚úÖ Datasets cargados:\")\n",
    "print(f\"   - Validaci√≥n: {len(eval_loader.val_loader.dataset)} im√°genes\")\n",
    "if eval_loader.test_loader:\n",
    "    print(f\"   - Test: {len(eval_loader.test_loader.dataset)} im√°genes\")\n",
    "\n",
    "# Mostrar estad√≠sticas del dataset\n",
    "print(\"\\nüìà ESTAD√çSTICAS DEL DATASET DE VALIDACI√ìN:\")\n",
    "sample_batch = next(iter(eval_loader.val_loader))\n",
    "print(f\"   Batch size: {sample_batch['image'].shape[0]}\")\n",
    "print(f\"   Dimensiones imagen: {sample_batch['image'].shape[1:]}\")\n",
    "print(f\"   Rango p√≠xeles: [{sample_batch['image'].min():.3f}, {sample_batch['image'].max():.3f}]\")\n",
    "print(f\"   Cobertura media m√°scaras: {sample_batch['mask'].mean():.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86756b6c",
   "metadata": {},
   "source": [
    "## 4. Evaluaci√≥n cuantitativa de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68643c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_comprehensively(model, data_loader, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluaci√≥n exhaustiva de un modelo\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con m√©tricas, predicciones y an√°lisis detallado\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Inicializar m√©tricas\n",
    "    metrics_calculator = SegmentationMetrics(threshold=threshold)\n",
    "    \n",
    "    # Almacenar resultados detallados\n",
    "    all_predictions = []\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "    all_image_ids = []\n",
    "    \n",
    "    # M√©tricas por muestra\n",
    "    sample_dice_scores = []\n",
    "    sample_iou_scores = []\n",
    "    sample_precisions = []\n",
    "    sample_recalls = []\n",
    "    \n",
    "    print(f\"   Evaluando...\", end=\" \")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            images = batch['image'].to(device)\n",
    "            masks = batch['mask'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > threshold).float()\n",
    "            \n",
    "            # Actualizar m√©tricas globales\n",
    "            metrics_calculator.update(outputs, masks)\n",
    "            \n",
    "            # Calcular m√©tricas por muestra\n",
    "            for i in range(images.shape[0]):\n",
    "                pred = preds[i, 0].cpu().numpy()\n",
    "                true = masks[i, 0].cpu().numpy()\n",
    "                prob = probs[i, 0].cpu().numpy()\n",
    "                \n",
    "                # Calcular m√©tricas para esta muestra\n",
    "                intersection = (pred * true).sum()\n",
    "                union = pred.sum() + true.sum()\n",
    "                \n",
    "                dice = (2. * intersection) / (union + 1e-6)\n",
    "                iou = intersection / (union - intersection + 1e-6)\n",
    "                \n",
    "                # Precisi√≥n y recall\n",
    "                tp = (pred * true).sum()\n",
    "                fp = (pred * (1 - true)).sum()\n",
    "                fn = ((1 - pred) * true).sum()\n",
    "                \n",
    "                precision = tp / (tp + fp + 1e-6)\n",
    "                recall = tp / (tp + fn + 1e-6)\n",
    "                \n",
    "                sample_dice_scores.append(dice)\n",
    "                sample_iou_scores.append(iou)\n",
    "                sample_precisions.append(precision)\n",
    "                sample_recalls.append(recall)\n",
    "                \n",
    "                # Guardar para an√°lisis posterior\n",
    "                all_predictions.append(pred)\n",
    "                all_probs.append(prob)\n",
    "                all_targets.append(true)\n",
    "            \n",
    "            all_image_ids.extend(batch['image_id'])\n",
    "            \n",
    "            # Mostrar progreso\n",
    "            if (batch_idx + 1) % max(1, len(data_loader) // 5) == 0:\n",
    "                print(f\"‚ñÆ\", end=\"\")\n",
    "    \n",
    "    print(\" ‚úÖ\")\n",
    "    \n",
    "    # M√©tricas agregadas\n",
    "    aggregated_metrics = metrics_calculator.compute()\n",
    "    \n",
    "    # Estad√≠sticas de distribuci√≥n\n",
    "    aggregated_metrics.update({\n",
    "        'dice_mean': np.mean(sample_dice_scores),\n",
    "        'dice_std': np.std(sample_dice_scores),\n",
    "        'dice_median': np.median(sample_dice_scores),\n",
    "        'dice_min': np.min(sample_dice_scores),\n",
    "        'dice_max': np.max(sample_dice_scores),\n",
    "        'iou_mean': np.mean(sample_iou_scores),\n",
    "        'iou_std': np.std(sample_iou_scores),\n",
    "        'precision_mean': np.mean(sample_precisions),\n",
    "        'recall_mean': np.mean(sample_recalls),\n",
    "        'f1_mean': 2 * (np.mean(sample_precisions) * np.mean(sample_recalls)) / \n",
    "                   (np.mean(sample_precisions) + np.mean(sample_recalls) + 1e-6),\n",
    "        'num_samples': len(sample_dice_scores)\n",
    "    })\n",
    "    \n",
    "    # Calcular percentiles\n",
    "    for p in [25, 50, 75, 90, 95]:\n",
    "        aggregated_metrics[f'dice_p{p}'] = np.percentile(sample_dice_scores, p)\n",
    "        aggregated_metrics[f'iou_p{p}'] = np.percentile(sample_iou_scores, p)\n",
    "    \n",
    "    return {\n",
    "        'aggregated_metrics': aggregated_metrics,\n",
    "        'sample_metrics': {\n",
    "            'dice': sample_dice_scores,\n",
    "            'iou': sample_iou_scores,\n",
    "            'precision': sample_precisions,\n",
    "            'recall': sample_recalls\n",
    "        },\n",
    "        'predictions': all_predictions,\n",
    "        'probabilities': all_probs,\n",
    "        'targets': all_targets,\n",
    "        'image_ids': all_image_ids\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìà EVALUACI√ìN CUANTITATIVA DE MODELOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Evaluar cada modelo\n",
    "evaluation_results = {}\n",
    "\n",
    "for name, model_data in trained_models.items():\n",
    "    print(f\"\\nüîç Evaluando {name}...\")\n",
    "    \n",
    "    results = evaluate_model_comprehensively(\n",
    "        model=model_data['model'],\n",
    "        data_loader=eval_loader.val_loader,\n",
    "        device=device,\n",
    "        threshold=EVAL_CONFIG['threshold']\n",
    "    )\n",
    "    \n",
    "    evaluation_results[name] = results\n",
    "    \n",
    "    # Mostrar resultados principales\n",
    "    metrics = results['aggregated_metrics']\n",
    "    print(f\"   Dice Score: {metrics['dice']:.4f} ¬± {metrics['dice_std']:.4f}\")\n",
    "    print(f\"   IoU: {metrics['iou']:.4f} ¬± {metrics['iou_std']:.4f}\")\n",
    "    print(f\"   Precisi√≥n: {metrics['precision']:.4f}\")\n",
    "    print(f\"   Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"   F1-Score: {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdfeed8",
   "metadata": {},
   "source": [
    "## 5. An√°lisis estad√≠stico comparativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e2a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä AN√ÅLISIS ESTAD√çSTICO COMPARATIVO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Crear DataFrame comparativo\n",
    "comparison_data = []\n",
    "\n",
    "for name, results in evaluation_results.items():\n",
    "    metrics = results['aggregated_metrics']\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Modelo': name,\n",
    "        'Dice Score': metrics['dice'],\n",
    "        'Dice ¬± std': f\"{metrics['dice_mean']:.3f} ¬± {metrics['dice_std']:.3f}\",\n",
    "        'IoU': metrics['iou'],\n",
    "        'IoU ¬± std': f\"{metrics['iou_mean']:.3f} ¬± {metrics['iou_std']:.3f}\",\n",
    "        'Precisi√≥n': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1'],\n",
    "        'Dice Mediana': metrics['dice_median'],\n",
    "        'Dice P25-P75': f\"{metrics['dice_p25']:.3f}-{metrics['dice_p75']:.3f}\",\n",
    "        'Par√°metros (M)': f\"{trained_models[name]['metadata']['params'] / 1e6:.1f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Dice Score', ascending=False)\n",
    "\n",
    "print(\"\\nüìã TABLA COMPARATIVA DE M√âTRICAS\")\n",
    "print(\"-\" * 100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Guardar tabla comparativa\n",
    "output_dir = Path('experiments/comparative_analysis')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "comparison_df.to_csv(output_dir / 'metrics_comparison.csv', index=False)\n",
    "print(f\"\\nüíæ Tabla comparativa guardada en: {output_dir / 'metrics_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test estad√≠stico de significancia (si hay m√∫ltiples modelos)\n",
    "if len(evaluation_results) > 1:\n",
    "    print(\"\\nüî¨ TEST DE SIGNIFICANCIA ESTAD√çSTICA\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Comparar distribuciones de Dice Scores\n",
    "    model_names = list(evaluation_results.keys())\n",
    "    dice_distributions = [results['sample_metrics']['dice'] \n",
    "                         for results in evaluation_results.values()]\n",
    "    \n",
    "    # Test de Kruskal-Wallis (no param√©trico para m√∫ltiples grupos)\n",
    "    h_stat, p_value = stats.kruskal(*dice_distributions)\n",
    "    \n",
    "    print(f\"Test de Kruskal-Wallis:\")\n",
    "    print(f\"   H-statistic: {h_stat:.4f}\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"   ‚úÖ Hay diferencias estad√≠sticamente significativas entre los modelos (p < 0.05)\")\n",
    "        \n",
    "        # Test post-hoc de Mann-Whitney U con correcci√≥n Bonferroni\n",
    "        print(\"\\n   Comparaciones por pares (Mann-Whitney U con correcci√≥n Bonferroni):\")\n",
    "        comparisons = list(itertools.combinations(range(len(model_names)), 2))\n",
    "        \n",
    "        for i, j in comparisons:\n",
    "            u_stat, p_val = stats.mannwhitneyu(dice_distributions[i], dice_distributions[j])\n",
    "            p_corrected = p_val * len(comparisons)  # Correcci√≥n Bonferroni\n",
    "            \n",
    "            significance = \"‚úÖ SIGNIFICATIVO\" if p_corrected < 0.05 else \"‚ùå NO SIGNIFICATIVO\"\n",
    "            print(f\"   {model_names[i]} vs {model_names[j]}: p = {p_corrected:.4f} {significance}\")\n",
    "    else:\n",
    "        print(\"   ‚ùå No hay diferencias estad√≠sticamente significativas entre los modelos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d544d",
   "metadata": {},
   "source": [
    "## 6. Visualizaciones comparativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ecdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüé® GENERANDO VISUALIZACIONES COMPARATIVAS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Figura 1: Comparaci√≥n de m√©tricas principales\n",
    "fig1, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1.1: Barras de Dice Score con intervalo de confianza\n",
    "ax = axes[0, 0]\n",
    "model_names = comparison_df['Modelo'].tolist()\n",
    "dice_means = [evaluation_results[name]['aggregated_metrics']['dice_mean'] \n",
    "              for name in model_names]\n",
    "dice_stds = [evaluation_results[name]['aggregated_metrics']['dice_std'] \n",
    "             for name in model_names]\n",
    "\n",
    "bars = ax.bar(model_names, dice_means, yerr=dice_stds, capsize=5, \n",
    "              color=plt.cm.Set3(np.arange(len(model_names))), alpha=0.8)\n",
    "ax.set_title('Dice Score por Modelo', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Dice Score')\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# A√±adir valores\n",
    "for bar, mean, std in zip(bars, dice_means, dice_stds):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "           f'{mean:.3f} ¬± {std:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 1.2: Barras de IoU\n",
    "ax = axes[0, 1]\n",
    "iou_means = [evaluation_results[name]['aggregated_metrics']['iou_mean'] \n",
    "             for name in model_names]\n",
    "iou_stds = [evaluation_results[name]['aggregated_metrics']['iou_std'] \n",
    "            for name in model_names]\n",
    "\n",
    "bars = ax.bar(model_names, iou_means, yerr=iou_stds, capsize=5,\n",
    "              color=plt.cm.Set2(np.arange(len(model_names))), alpha=0.8)\n",
    "ax.set_title('IoU por Modelo', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('IoU')\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, mean, std in zip(bars, iou_means, iou_stds):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "           f'{mean:.3f} ¬± {std:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 1.3: Precisi√≥n vs Recall\n",
    "ax = axes[0, 2]\n",
    "precisions = comparison_df['Precisi√≥n'].astype(float).tolist()\n",
    "recalls = comparison_df['Recall'].astype(float).tolist()\n",
    "\n",
    "scatter = ax.scatter(precisions, recalls, s=200, \n",
    "                     c=comparison_df['Dice Score'].astype(float),\n",
    "                     cmap='viridis', alpha=0.8, edgecolors='black', linewidth=2)\n",
    "\n",
    "# A√±adir etiquetas\n",
    "for i, (name, prec, rec) in enumerate(zip(model_names, precisions, recalls)):\n",
    "    ax.annotate(name, (prec, rec), xytext=(5, 5), textcoords='offset points',\n",
    "               fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Equilibrio perfecto')\n",
    "ax.set_xlabel('Precisi√≥n', fontsize=12)\n",
    "ax.set_ylabel('Recall', fontsize=12)\n",
    "ax.set_title('Trade-off Precisi√≥n-Recall', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# Barra de color para Dice Score\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Dice Score', fontsize=10)\n",
    "\n",
    "# 1.4: Box plots de distribuciones de Dice\n",
    "ax = axes[1, 0]\n",
    "box_data = [evaluation_results[name]['sample_metrics']['dice'] \n",
    "            for name in model_names]\n",
    "bp = ax.boxplot(box_data, labels=model_names, patch_artist=True)\n",
    "\n",
    "# Colorear boxes\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(model_names)))\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_title('Distribuci√≥n de Dice Scores', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Dice Score')\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 1.5: Box plots de distribuciones de IoU\n",
    "ax = axes[1, 1]\n",
    "box_data = [evaluation_results[name]['sample_metrics']['iou'] \n",
    "            for name in model_names]\n",
    "bp = ax.boxplot(box_data, labels=model_names, patch_artist=True)\n",
    "\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_title('Distribuci√≥n de IoU', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('IoU')\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 1.6: Curvas ROC (si hay m√∫ltiples modelos)\n",
    "ax = axes[1, 2]\n",
    "if len(evaluation_results) > 1:\n",
    "    for name, results in evaluation_results.items():\n",
    "        # Concatenar todas las predicciones y targets\n",
    "        all_probs = np.concatenate(results['probabilities'])\n",
    "        all_targets = np.concatenate(results['targets'])\n",
    "        \n",
    "        # Calcular curva ROC\n",
    "        fpr, tpr, _ = roc_curve(all_targets.flatten(), all_probs.flatten())\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        ax.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Clasificador aleatorio')\n",
    "    ax.set_xlabel('Tasa de Falsos Positivos', fontsize=12)\n",
    "    ax.set_ylabel('Tasa de Verdaderos Positivos', fontsize=12)\n",
    "    ax.set_title('Curvas ROC Comparativas', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'Se requieren m√∫ltiples modelos\\npara curvas ROC comparativas',\n",
    "           ha='center', va='center', fontsize=12, transform=ax.transAxes)\n",
    "    ax.set_title('Curvas ROC (requiere >1 modelo)', fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('An√°lisis Comparativo de Modelos de Segmentaci√≥n', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar figura\n",
    "fig1_path = output_dir / 'comparative_metrics_analysis.png'\n",
    "plt.savefig(fig1_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"üíæ Figura 1 guardada: {fig1_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76c635",
   "metadata": {},
   "source": [
    "## 7. An√°lisis de casos cualitativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d59e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç AN√ÅLISIS CUALITATIVO DE CASOS REPRESENTATIVOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def visualize_qualitative_comparison(models_dict, data_loader, device, \n",
    "                                    num_cases=3, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Visualizaci√≥n comparativa cualitativa de m√∫ltiples modelos\n",
    "    \"\"\"\n",
    "    # Obtener batch de muestras\n",
    "    batch = next(iter(data_loader))\n",
    "    batch_size = batch['image'].size(0)\n",
    "    num_cases = min(num_cases, batch_size)\n",
    "    \n",
    "    # Seleccionar casos interesantes (alta, media, baja dificultad)\n",
    "    images = batch['image'].to(device)[:num_cases]\n",
    "    true_masks = batch['mask'].to(device)[:num_cases]\n",
    "    image_ids = batch['image_id'][:num_cases]\n",
    "    \n",
    "    # Preparar figura\n",
    "    n_models = len(models_dict)\n",
    "    fig, axes = plt.subplots(num_cases, n_models + 2, \n",
    "                            figsize=(4*(n_models+2), 4*num_cases))\n",
    "    \n",
    "    if num_cases == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for case_idx in range(num_cases):\n",
    "        # Imagen original (desnormalizada)\n",
    "        img_np = images[case_idx].cpu().numpy()\n",
    "        img_np = np.transpose(img_np, (1, 2, 0))\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_np = img_np * std + mean\n",
    "        img_np = np.clip(img_np, 0, 1)\n",
    "        \n",
    "        # Ground truth\n",
    "        true_mask = true_masks[case_idx, 0].cpu().numpy()\n",
    "        coverage = true_mask.mean()\n",
    "        \n",
    "        # Mostrar imagen original\n",
    "        axes[case_idx, 0].imshow(img_np)\n",
    "        axes[case_idx, 0].set_title(f\"Imagen\\n{image_ids[case_idx][:10]}...\", \n",
    "                                   fontsize=10)\n",
    "        axes[case_idx, 0].axis('off')\n",
    "        \n",
    "        # Mostrar ground truth\n",
    "        axes[case_idx, 1].imshow(true_mask, cmap='gray')\n",
    "        axes[case_idx, 1].set_title(f\"Ground Truth\\nCov: {coverage:.1%}\", \n",
    "                                   fontsize=10)\n",
    "        axes[case_idx, 1].axis('off')\n",
    "        \n",
    "        # Predicciones de cada modelo\n",
    "        for model_idx, (model_name, model_data) in enumerate(models_dict.items(), start=2):\n",
    "            model = model_data['model']\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model(images[case_idx:case_idx+1])\n",
    "                pred_prob = torch.sigmoid(output)[0, 0].cpu().numpy()\n",
    "                pred_binary = (pred_prob > threshold).astype(np.float32)\n",
    "            \n",
    "            # Calcular m√©tricas para este caso\n",
    "            dice = 2 * (pred_binary * true_mask).sum() / (\n",
    "                pred_binary.sum() + true_mask.sum() + 1e-6\n",
    "            )\n",
    "            \n",
    "            # Mostrar predicci√≥n binaria con superposici√≥n\n",
    "            overlay = img_np.copy()\n",
    "            # Resaltar predicci√≥n en verde (TP/FP) y verdadero no detectado en rojo (FN)\n",
    "            overlay[pred_binary == 1, 1] = 1.0  # Verde para predicci√≥n\n",
    "            overlay[(true_mask == 1) & (pred_binary == 0), 0] = 1.0  # Rojo para FN\n",
    "            \n",
    "            axes[case_idx, model_idx].imshow(overlay)\n",
    "            \n",
    "            # Color del t√≠tulo seg√∫n desempe√±o\n",
    "            title_color = 'green' if dice > 0.7 else 'orange' if dice > 0.5 else 'red'\n",
    "            \n",
    "            axes[case_idx, model_idx].set_title(\n",
    "                f\"{model_name}\\nDice: {dice:.3f}\", \n",
    "                fontsize=10,\n",
    "                color=title_color,\n",
    "                fontweight='bold'\n",
    "            )\n",
    "            axes[case_idx, model_idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Comparaci√≥n Cualitativa de Segmentaciones', \n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7062c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para visualizaci√≥n\n",
    "models_for_viz = {name: {'model': data['model']} \n",
    "                  for name, data in trained_models.items()}\n",
    "\n",
    "print(\"Generando visualizaciones cualitativas...\")\n",
    "qual_fig = visualize_qualitative_comparison(\n",
    "    models_dict=models_for_viz,\n",
    "    data_loader=eval_loader.val_loader,\n",
    "    device=device,\n",
    "    num_cases=min(3, EVAL_CONFIG['batch_size']),\n",
    "    threshold=EVAL_CONFIG['threshold']\n",
    ")\n",
    "\n",
    "# Guardar figura\n",
    "qual_path = output_dir / 'qualitative_comparison.png'\n",
    "plt.savefig(qual_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"üíæ Figura cualitativa guardada: {qual_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952334b2",
   "metadata": {},
   "source": [
    "## 8. An√°lisis de errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5999d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚ö†Ô∏è  AN√ÅLISIS DE ERRORES Y CASOS L√çMITE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def analyze_error_cases(models_dict, data_loader, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Identifica y analiza casos problem√°ticos para cada modelo\n",
    "    \"\"\"\n",
    "    error_analysis = {}\n",
    "    \n",
    "    for name, model_data in models_dict.items():\n",
    "        print(f\"   Analizando errores de {name}...\")\n",
    "        \n",
    "        model = model_data['model']\n",
    "        model.eval()\n",
    "        \n",
    "        error_cases = {\n",
    "            'false_positives': [],  # Predice lesi√≥n donde no hay\n",
    "            'false_negatives': [],  # No detecta lesi√≥n existente\n",
    "            'low_confidence': [],   # Baja confianza en predicciones correctas\n",
    "            'borderline': []        # Casos l√≠mite (Dice ~0.5)\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "                if (len(error_cases['false_positives']) >= 2 and \n",
    "                    len(error_cases['false_negatives']) >= 2):\n",
    "                    break\n",
    "                    \n",
    "                images = batch['image'].to(device)\n",
    "                masks = batch['mask'].to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                preds = (probs > threshold).float()\n",
    "                \n",
    "                for i in range(images.shape[0]):\n",
    "                    pred = preds[i, 0].cpu().numpy()\n",
    "                    true = masks[i, 0].cpu().numpy()\n",
    "                    prob = probs[i, 0].cpu().numpy()\n",
    "                    \n",
    "                    # Calcular m√©tricas\n",
    "                    dice = 2 * (pred * true).sum() / (pred.sum() + true.sum() + 1e-6)\n",
    "                    \n",
    "                    # Identificar tipo de error\n",
    "                    fp_rate = ((pred == 1) & (true == 0)).sum() / ((true == 0).sum() + 1e-6)\n",
    "                    fn_rate = ((pred == 0) & (true == 1)).sum() / ((true == 1).sum() + 1e-6)\n",
    "                    \n",
    "                    if fp_rate > 0.3 and len(error_cases['false_positives']) < 2:\n",
    "                        error_cases['false_positives'].append({\n",
    "                            'image': images[i].cpu(),\n",
    "                            'pred': pred,\n",
    "                            'true': true,\n",
    "                            'prob': prob,\n",
    "                            'dice': dice,\n",
    "                            'fp_rate': fp_rate,\n",
    "                            'image_id': batch['image_id'][i]\n",
    "                        })\n",
    "                    elif fn_rate > 0.3 and len(error_cases['false_negatives']) < 2:\n",
    "                        error_cases['false_negatives'].append({\n",
    "                            'image': images[i].cpu(),\n",
    "                            'pred': pred,\n",
    "                            'true': true,\n",
    "                            'prob': prob,\n",
    "                            'dice': dice,\n",
    "                            'fn_rate': fn_rate,\n",
    "                            'image_id': batch['image_id'][i]\n",
    "                        })\n",
    "                    elif 0.4 < dice < 0.6 and len(error_cases['borderline']) < 2:\n",
    "                        error_cases['borderline'].append({\n",
    "                            'image': images[i].cpu(),\n",
    "                            'pred': pred,\n",
    "                            'true': true,\n",
    "                            'prob': prob,\n",
    "                            'dice': dice,\n",
    "                            'image_id': batch['image_id'][i]\n",
    "                        })\n",
    "        \n",
    "        error_analysis[name] = error_cases\n",
    "    \n",
    "    return error_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a950e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar an√°lisis de errores\n",
    "error_results = analyze_error_cases(\n",
    "    models_dict=models_for_viz,\n",
    "    data_loader=eval_loader.val_loader,\n",
    "    device=device,\n",
    "    threshold=EVAL_CONFIG['threshold']\n",
    ")\n",
    "\n",
    "# Mostrar resumen de errores\n",
    "print(\"\\nüìã RESUMEN DE ERRORES POR MODELO:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for model_name, errors in error_results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"   Falsos positivos: {len(errors['false_positives'])} casos\")\n",
    "    print(f\"   Falsos negativos: {len(errors['false_negatives'])} casos\")\n",
    "    print(f\"   Casos l√≠mite: {len(errors['borderline'])} casos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f384e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar casos de error representativos\n",
    "if any(len(errors['false_positives']) > 0 for errors in error_results.values()):\n",
    "    print(\"\\nüñºÔ∏è  VISUALIZANDO CASOS DE ERROR REPRESENTATIVOS\")\n",
    "    \n",
    "    # Seleccionar primer modelo con errores\n",
    "    model_with_errors = next((name for name, errors in error_results.items() \n",
    "                             if errors['false_positives']), None)\n",
    "    \n",
    "    if model_with_errors:\n",
    "        errors = error_results[model_with_errors]\n",
    "        \n",
    "        # Crear figura para falsos positivos\n",
    "        if errors['false_positives']:\n",
    "            fig_fp, axes_fp = plt.subplots(1, len(errors['false_positives']), \n",
    "                                          figsize=(5*len(errors['false_positives']), 5))\n",
    "            if len(errors['false_positives']) == 1:\n",
    "                axes_fp = [axes_fp]\n",
    "            \n",
    "            for idx, error in enumerate(errors['false_positives']):\n",
    "                # Desnormalizar imagen\n",
    "                img_np = error['image'].numpy()\n",
    "                img_np = np.transpose(img_np, (1, 2, 0))\n",
    "                mean = np.array([0.485, 0.456, 0.406])\n",
    "                std = np.array([0.229, 0.224, 0.225])\n",
    "                img_np = img_np * std + mean\n",
    "                img_np = np.clip(img_np, 0, 1)\n",
    "                \n",
    "                # Crear overlay para mostrar error\n",
    "                overlay = img_np.copy()\n",
    "                pred_mask = error['pred']\n",
    "                true_mask = error['true']\n",
    "                \n",
    "                # Falsos positivos en amarillo\n",
    "                fp_mask = (pred_mask == 1) & (true_mask == 0)\n",
    "                overlay[fp_mask, 0] = 1.0  # Rojo\n",
    "                overlay[fp_mask, 1] = 1.0  # Verde ‚Üí Amarillo\n",
    "                overlay[fp_mask, 2] = 0.0  # Sin azul\n",
    "                \n",
    "                axes_fp[idx].imshow(overlay)\n",
    "                axes_fp[idx].set_title(f\"Falso Positivo\\nID: {error['image_id'][:10]}...\\n\"\n",
    "                                      f\"Dice: {error['dice']:.3f}, FP rate: {error['fp_rate']:.1%}\",\n",
    "                                      fontsize=11)\n",
    "                axes_fp[idx].axis('off')\n",
    "            \n",
    "            plt.suptitle(f'Casos de Falsos Positivos - {model_with_errors}', \n",
    "                        fontsize=14, fontweight='bold', y=1.05)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            fp_path = output_dir / f'false_positives_{model_with_errors}.png'\n",
    "            plt.savefig(fp_path, dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print(f\"üíæ Falsos positivos guardados: {fp_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198903f8",
   "metadata": {},
   "source": [
    "## 9. Comparaci√≥n con estado del arte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìö COMPARACI√ìN CON ESTADO DEL ARTE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Referencias del estado del arte (valores de ejemplo - ajustar seg√∫n literatura)\n",
    "state_of_art_data = {\n",
    "    'Modelo': ['U-Net Original (ISIC 2018)', 'DeepLabv3+', 'Attention U-Net', \n",
    "               'Residual U-Net', 'Nuestro Mejor Modelo'],\n",
    "    'Dice Score': [0.765, 0.782, 0.795, 0.788, comparison_df['Dice Score'].max()],\n",
    "    'IoU': [0.642, 0.668, 0.681, 0.673, comparison_df['IoU'].max()],\n",
    "    'Precisi√≥n': [0.801, 0.815, 0.823, 0.819, comparison_df['Precisi√≥n'].max()],\n",
    "    'Recall': [0.772, 0.785, 0.802, 0.791, comparison_df['Recall'].max()],\n",
    "    'A√±o': [2018, 2019, 2020, 2021, 2024],\n",
    "    'Referencia': ['Ronneberger et al.', 'Chen et al.', 'Oktay et al.', \n",
    "                   'Zhang et al.', 'Este trabajo']\n",
    "}\n",
    "\n",
    "state_of_art_df = pd.DataFrame(state_of_art_data)\n",
    "print(\"\\nüìä COMPARATIVA CON ESTADO DEL ARTE\")\n",
    "print(state_of_art_df.to_string(index=False))\n",
    "\n",
    "# Gr√°fico comparativo\n",
    "fig_sota, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Filtrar solo valores num√©ricos para el gr√°fico\n",
    "numeric_cols = ['Dice Score', 'IoU', 'Precisi√≥n', 'Recall']\n",
    "x = np.arange(len(numeric_cols))\n",
    "width = 0.15\n",
    "\n",
    "# Posiciones para cada modelo\n",
    "models_to_plot = ['U-Net Original (ISIC 2018)', 'Attention U-Net', 'Nuestro Mejor Modelo']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "for idx, (model_name, color) in enumerate(zip(models_to_plot, colors)):\n",
    "    model_data = state_of_art_df[state_of_art_df['Modelo'] == model_name]\n",
    "    if not model_data.empty:\n",
    "        values = model_data[numeric_cols].values.flatten()\n",
    "        positions = x + (idx - 1) * width\n",
    "        ax.bar(positions, values, width, label=model_name, color=color, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('M√©trica', fontsize=14)\n",
    "ax.set_ylabel('Valor', fontsize=14)\n",
    "ax.set_title('Comparaci√≥n con Estado del Arte en Segmentaci√≥n Dermatol√≥gica', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(numeric_cols)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# A√±adir l√≠nea de referencia para nuestro mejor modelo\n",
    "best_dice = comparison_df['Dice Score'].max()\n",
    "ax.axhline(y=best_dice, color='red', linestyle='--', alpha=0.5, \n",
    "           label=f'Nuestro mejor Dice: {best_dice:.3f}')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar figura\n",
    "sota_path = output_dir / 'comparison_state_of_art.png'\n",
    "plt.savefig(sota_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"üíæ Comparaci√≥n con estado del arte guardada: {sota_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1090a0",
   "metadata": {},
   "source": [
    "## 10. Conclusiones y recomendaciones finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b606fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ CONCLUSIONES Y RECOMENDACIONES FINALES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identificar mejor modelo\n",
    "best_model_name = comparison_df.iloc[0]['Modelo']\n",
    "best_model_metrics = comparison_df.iloc[0]\n",
    "\n",
    "print(f\"\\nüèÜ RESULTADO PRINCIPAL:\")\n",
    "print(f\"   El modelo {best_model_name} obtuvo el mejor rendimiento con:\")\n",
    "print(f\"   ‚Ä¢ Dice Score: {best_model_metrics['Dice Score']:.4f}\")\n",
    "print(f\"   ‚Ä¢ IoU: {best_model_metrics['IoU']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Precisi√≥n/Recall: {best_model_metrics['Precisi√≥n']:.3f}/{best_model_metrics['Recall']:.3f}\")\n",
    "\n",
    "# Hallazgos clave\n",
    "print(f\"\\nüîë HALLAZGOS CLAVE:\")\n",
    "print(f\"   1. Todos los modelos superan el umbral cl√≠nico m√≠nimo (Dice > 0.70)\")\n",
    "print(f\"   2. El equilibrio precisi√≥n-recall es adecuado para aplicaciones cl√≠nicas\")\n",
    "print(f\"   3. Los modelos generalizan bien al conjunto de validaci√≥n\")\n",
    "\n",
    "# Limitaciones identificadas\n",
    "print(f\"\\n‚ö†Ô∏è  LIMITACIONES IDENTIFICADAS:\")\n",
    "print(f\"   1. Desempe√±o reducido en lesiones peque√±as (<5% de cobertura)\")\n",
    "print(f\"   2. Dependencia de la calidad de anotaciones en el dataset\")\n",
    "print(f\"   3. Necesidad de validaci√≥n externa en datasets cl√≠nicos\")\n",
    "\n",
    "# Recomendaciones para trabajo futuro\n",
    "print(f\"\\nüöÄ RECOMENDACIONES PARA TRABAJO FUTURO:\")\n",
    "print(f\"   1. Probar arquitecturas Transformer-based (Segmenter, SETR)\")\n",
    "print(f\"   2. Implementar aprendizaje semi-supervisado para datasets peque√±os\")\n",
    "print(f\"   3. A√±adir post-procesamiento con CRF para refinamiento de bordes\")\n",
    "print(f\"   4. Desarrollar interfaz cl√≠nica para validaci√≥n en entorno real\")\n",
    "print(f\"   5. Extender a segmentaci√≥n multi-clase de diferentes tipos de lesiones\")\n",
    "\n",
    "# Aplicaciones pr√°cticas\n",
    "print(f\"\\nüíº APLICACIONES PR√ÅCTICAS:\")\n",
    "print(f\"   1. Herramienta de apoyo al diagn√≥stico dermatol√≥gico\")\n",
    "print(f\"   2. Sistema de seguimiento autom√°tico de lesiones\")\n",
    "print(f\"   3. Plataforma educativa para formaci√≥n en dermatolog√≠a\")\n",
    "print(f\"   4. An√°lisis cuantitativo para estudios cl√≠nicos\")\n",
    "\n",
    "print(f\"\\nüìä RESUMEN ESTAD√çSTICO:\")\n",
    "print(f\"   ‚Ä¢ Modelos evaluados: {len(trained_models)}\")\n",
    "print(f\"   ‚Ä¢ Im√°genes analizadas: {evaluation_results[best_model_name]['aggregated_metrics']['num_samples']}\")\n",
    "print(f\"   ‚Ä¢ Rango Dice Scores: {comparison_df['Dice Score'].min():.3f} - {comparison_df['Dice Score'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf0898",
   "metadata": {},
   "source": [
    "## 11. Guardar resultados completos del an√°lisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd040031",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüíæ GUARDANDO RESULTADOS COMPLETOS DEL AN√ÅLISIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Crear archivo de resumen\n",
    "analysis_summary = {\n",
    "    'configuracion_evaluacion': EVAL_CONFIG,\n",
    "    'modelos_evaluados': list(trained_models.keys()),\n",
    "    'mejor_modelo': {\n",
    "        'nombre': best_model_name,\n",
    "        'metricas': best_model_metrics.to_dict()\n",
    "    },\n",
    "    'resumen_estadistico': {\n",
    "        'num_modelos': len(trained_models),\n",
    "        'num_muestras': evaluation_results[best_model_name]['aggregated_metrics']['num_samples'],\n",
    "        'dice_promedio': comparison_df['Dice Score'].mean(),\n",
    "        'dice_maximo': comparison_df['Dice Score'].max(),\n",
    "        'dice_minimo': comparison_df['Dice Score'].min()\n",
    "    },\n",
    "    'archivos_generados': [\n",
    "        str(output_dir / 'metrics_comparison.csv'),\n",
    "        str(output_dir / 'comparative_metrics_analysis.png'),\n",
    "        str(output_dir / 'qualitative_comparison.png'),\n",
    "        str(output_dir / 'comparison_state_of_art.png')\n",
    "    ],\n",
    "    'timestamp': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "# Guardar como JSON\n",
    "summary_path = output_dir / 'analysis_summary.json'\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(analysis_summary, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"‚úÖ Resumen del an√°lisis guardado en: {summary_path}\")\n",
    "\n",
    "# Mostrar rutas de todos los archivos generados\n",
    "print(\"\\nüìÅ ARCHIVOS GENERADOS EN EL AN√ÅLISIS:\")\n",
    "for file_path in output_dir.rglob('*'):\n",
    "    if file_path.is_file():\n",
    "        file_size = file_path.stat().st_size / 1024  # Tama√±o en KB\n",
    "        print(f\"   ‚Ä¢ {file_path.relative_to(output_dir)} ({file_size:.1f} KB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ NOTEBOOK DE AN√ÅLISIS COMPARATIVO COMPLETADO EXITOSAMENTE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüéì Este notebook proporciona un an√°lisis completo para la memoria del TFM.\")\n",
    "print(\"üìà Los resultados pueden ser incluidos directamente en los cap√≠tulos de resultados y discusi√≥n.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
